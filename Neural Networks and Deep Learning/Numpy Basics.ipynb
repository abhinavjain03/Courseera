{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1/(1+math.exp(-1*x))\n",
    "    return s\n",
    "\n",
    "sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-1*x))\n",
    "    return s\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    ds = s*(1-s)\n",
    "    return ds\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "def image2vector(image):\n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    return v\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    x = x/x_norm\n",
    "    return x\n",
    "\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims = True)\n",
    "    s = x_exp/x_sum\n",
    "    return s\n",
    "\n",
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "* np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
    "* the sigmoid function and its gradient\n",
    "* image2vector is commonly used in deep learning\n",
    "* np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.\n",
    "* numpy has efficient built-in functions\n",
    "* broadcasting is extremely useful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat, y):\n",
    "    loss = np.sum(np.abs(yhat-y))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "def L2(yhat, y):\n",
    "    loss = np.sum(np.dot((y-yhat),(y-yhat)))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takeaways:\n",
    "* Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
    "* You have reviewed the L1 and L2 loss\n",
    "* You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
